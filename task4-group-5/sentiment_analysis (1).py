# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dLanJq9S2cqoAZXEzBVI5Fq_Zr2UKVyc
"""

import numpy as np
import pandas as pd
import seaborn as sns
import nltk
import re
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score
from mlxtend.plotting import plot_confusion_matrix
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from mlxtend.plotting import plot_confusion_matrix
import matplotlib.ticker as ticker

"""# **Load Dataset by pandas**"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('/content/reviews.csv')


df.head(10)

print(df.columns)

print("Data Types:")
print(df.dtypes)

# Correcting the wrong datatype (Turning object time column into datetime format)
df.Time_submitted = pd.to_datetime(df.Time_submitted)

print(df.isna().sum())

# Create a new column based on length of the reviews
df['length_of_text'] = [len(i.split(' ')) for i in df['Review']]

# Let's look at the summary of the dataset
print(df.head())

# Let's look at the summary of the dataset.
# Ratings are integers in range of 1 and 5.
# Maximum text length has 699 words in it, and minimum is 2 words.
# Reviews with maximum thumbs up has 8195 thumbs up. And most of the reviews have no thumbs up.
print(df.describe())

"""Data Visualizations: Part I"""

import plotly.express as px

# Distribution of the Length of the Reviews (Length of Text greater than 120 is neglected)
fig = px.histogram(df[df['length_of_text'] <= 120].length_of_text, marginal='box',
                   labels={"value": "Length of the Reviews"})
fig.update_traces(marker=dict(line=dict(color='#000000', width=0.25)))
fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'})
fig.update_layout(title_text='Distribution of the Length of the Reviews',
                  title_x=0.5, title_font=dict(size=18), showlegend=False)
fig.show()

# Text length mean, std, number of ratings and total words for each Ratings
print(df.groupby('Rating').length_of_text.agg({'mean', 'std', 'count', 'sum'}))

# Distribution of the Length of the Reviews by their Ratings (Length of Text greater than 120 is neglected)
fig = px.histogram(df[df['length_of_text'] <= 120], x='length_of_text', color='Rating',
                   marginal='box', labels={"value": "Length of the Reviews"})
fig.update_traces(marker=dict(line=dict(color='#000000', width=0.25)))
fig.update_layout(title_text='Distribution of the Length of the Reviews by their Ratings',
                  title_x=0.5, title_font=dict(size=18))
fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'})
fig.show()

# Frequency of the Ratings
fig = px.histogram(df, x='Rating', color='Rating')
fig.update_traces(marker=dict(line=dict(color='#000000', width=0.5)))
fig.update_layout(title_text='Frequency of the Ratings',
                  title_x=0.5, title_font=dict(size=18))
fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'})
fig.update_layout(bargap=0.2)
fig.show()

import plotly.graph_objects as go

# Number of Reviews Over Time
dailyNumOfReviews = df.resample('d', on='Time_submitted').size().sort_index()

fig = go.Figure()
fig.add_trace(go.Scatter(y=dailyNumOfReviews, x=dailyNumOfReviews.index,
                         mode='markers+text+lines', name=f"Number of Reviews", line=dict(width=3)))

fig.update_layout(
    yaxis=dict(title_text="Number of Reviews", titlefont=dict(size=15)),
    xaxis=dict(title_text="Date", titlefont=dict(size=15)),
    title={'text': f"Number of Reviews Over Time",
           'x': 0.5})
fig.update_traces(marker=dict(line=dict(color='#000000', width=0.75)))
fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'})
fig.show()

# Number of Reviews Over Time for each Rating
dailyNumOfReviewsEachRatings = df.groupby('Rating').resample('d', on='Time_submitted').size().unstack().T

fig = go.Figure()
for col in dailyNumOfReviewsEachRatings.columns:
    fig.add_trace(go.Scatter(y=dailyNumOfReviewsEachRatings[col],
                             x=dailyNumOfReviewsEachRatings.index,
                             mode='lines',
                             name=f"Number of Reviews for Rating {col}",
                             line=dict(width=1.5)))

fig.update_layout(
    yaxis=dict(title_text="Number of Reviews", titlefont=dict(size=15)),
    xaxis=dict(title_text="Date", titlefont=dict(size=15)),
    title={'text': f"Number of Reviews Over Time for each Rating",
           'x': 0.5})
fig.update_traces(marker=dict(line=dict(color='#000000', width=1.2)))
fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'})
fig.show()

# Rolling Mean of the Rating over Time (7 days window)
RatingDaily = df.sort_values('Time_submitted').resample('d', on='Time_submitted').Rating.mean().to_frame()
rollingRating = RatingDaily.rolling(7, min_periods=2).Rating.mean()

fig = go.Figure()
fig.add_trace(go.Scatter(y=rollingRating, x=rollingRating.index,
                         mode='markers+lines', name=f"Rolling Mean of the Ratings", line=dict(width=3)))

fig.update_layout(
    yaxis=dict(title_text="Average Rating", titlefont=dict(size=15)),
    xaxis=dict(title_text="Date", titlefont=dict(size=15)),
    title={'text': f"Rolling Mean of the Rating over Time (7 days window)",
           'x': 0.5})
fig.update_traces(marker=dict(line=dict(color='#000000', width=0.75)))
fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'})
fig.show()

import plotly.express as px

# Top used 100 Words before Text Cleaning
FreqOfWords = df['Review'].str.split(expand=True).stack().value_counts()
FreqOfWords_top100 = FreqOfWords[:100].reset_index()
FreqOfWords_top100.columns = ['Word', 'Count']

fig = px.treemap(FreqOfWords_top100, path=['Word'], values='Count')
fig.update_layout(title_text='Top used 100 Words before Text Cleaning',
                  title_x=0.5, title_font=dict(size=18))
fig.show()

"""Text Cleaning"""

# Lower case of the Reviews.
df['Review'] = df['Review'].str.lower()

!pip install contractions

import contractions

# Fix contractions such as doesn't to does not, he's to he is, etc.
def fixContractions(inputs):
    return contractions.fix(inputs)

df['ReviewContractions'] = df['Review'].apply(fixContractions)

# Some examples
print(df['Review'][4])
print(df['ReviewContractions'][4])

# Remove Numbers
df.ReviewContractions = df.ReviewContractions.replace(r'\d+', '', regex=True)

!pip install nltk

import nltk
nltk.download('punkt')

import nltk
from nltk.tokenize import word_tokenize

# Download NLTK punkt tokenizer
nltk.download('punkt')

# Tokenization
def tokenization(inputs):
    return word_tokenize(inputs)

df['ReviewTokenized'] = df['ReviewContractions'].apply(tokenization)
print(df.ReviewTokenized[5])
print(df.ReviewTokenized[600])

from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

# Stopwords Removal
stop_words = set(stopwords.words('english'))
stop_words.remove('not')  # 'not' is usually meaningful, so it's removed from stopwords

def stopwordsRemove(inputs):
    return [item for item in inputs if item not in stop_words]

df['ReviewStop'] = df['ReviewTokenized'].apply(stopwordsRemove)
print(df.ReviewStop[5])
print(df.ReviewStop[600])

import re

# Remove punctuations from tokenized text rows
def removePunctuation(inputs):
    p = re.compile(r'[^\w\s]+')
    return p.sub('', inputs)

df['ReviewStop'] = df['ReviewStop'].apply(lambda x: list(map(removePunctuation, x)))
print(df.ReviewStop[5])
print(df.ReviewStop[600])

import re

# Removing Emojis from the text
def removeEmoji(string):
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002500-\U00002BEF"  # chinese char
                               u"\U00002702-\U000027B0"
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               u"\U0001f926-\U0001f937"
                               u"\U00010000-\U0010ffff"
                               u"\u2640-\u2642"
                               u"\u2600-\u2B55"
                               u"\u200d"
                               u"\u23cf"
                               u"\u23e9"
                               u"\u231a"
                               u"\ufe0f"  # dingbats
                               u"\u3030"
                               "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', string)

df['ReviewStop'] = df['ReviewStop'].apply(lambda x: list(map(removeEmoji, x)))
print(df.ReviewStop[5])
print(df.ReviewStop[600])

import re

# Remove non-latin words from the sentences
def removeNonLatin(string):
    return re.sub(r'[^\x00-\x7f]', '', string)

# Remove underscores
def removeUnderscores(string):
    return string.replace('_', '')

df['ReviewStop'] = df['ReviewStop'].apply(lambda x: list(map(removeNonLatin, x)))
df['ReviewStop'] = df['ReviewStop'].apply(lambda x: list(map(removeUnderscores, x)))

# Removing Words less than length 2. Because I replaced emojis and punctuations with ''.
# There are still in the text as a blank element of the sentence. After this step, they will be removed.
def removeLessThan_2(inputs):
    return [j for j in inputs if len(j) > 2]


df['ReviewStop'] = df['ReviewStop'].apply(removeLessThan_2)

import nltk
nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer

# Initialize the WordNet Lemmatizer
lemmatizer = WordNetLemmatizer()

# Lemmatization
def lemmatization(inputs):
    return [lemmatizer.lemmatize(word=x, pos='v') for x in inputs]

df['ReviewLemmatized'] = df['ReviewStop'].apply(lemmatization)
print(df.ReviewLemmatized[5])
print(df.ReviewLemmatized[600])

# Joining Tokens into Sentences
df['ReviewFinal'] = df['ReviewLemmatized'].str.join(' ')
print(df.ReviewFinal[5])
print(df.ReviewFinal[600])

print(FreqOfWords_top100)



import pandas as pd
import plotly.express as px


# Adjust column names
df_top100.columns = ['word', 'count']

# Plot the treemap
fig = px.treemap(df_top100, path=['word'], values='count')
fig.update_layout(title_text='Top used 100 Words after Text Cleaning',
                  title_x=0.5, title_font=dict(size=18))
fig.show()

print(df.columns)

# Top used Words at Drop and Peak Dates of the Rating Rolling Average, what happened between these time periods?
# As discussed previously, we assumed that there was some major bugs or something went wrong for these time periods.
# (12 April - 17 April) = pt1, (27 April - 7 May) = pt2, and (8 March - 14 March) = pt3
# Top used Words at Drop and Peak Dates of the Rating Rolling Average, what happened between these time periods?
# As discussed previously, we assumed that there were some major bugs or something went wrong for these time periods.
# (12 April - 17 April) = pt1, (27 April - 7 May) = pt2, and (8 March - 14 March) = pt3

pt1_data = df[(df['Time_submitted'] >= '2022-04-12 00:00:00') & (df['Time_submitted'] <= '2022-04-17 23:59:59')]
pt2_data = df[(df['Time_submitted'] >= '2022-04-27 00:00:00') & (df['Time_submitted'] <= '2022-05-07 23:59:59')]
pt3_data = df[(df['Time_submitted'] >= '2022-03-08 00:00:00') & (df['Time_submitted'] <= '2022-03-14 23:59:59')]

# Create ngrams
def get_ngrams(text_input, n):  # https://stackoverflow.com/a/32307986
    n_grams = ngrams(word_tokenize(text_input), n)
    return [' '.join(grams) for grams in n_grams]

import plotly.express as px

# Select only numeric columns
numeric_df = df.select_dtypes(include=['int64', 'float64'])

fig = px.imshow(numeric_df.corr().round(2), text_auto=True, aspect="auto",
                color_continuous_scale='Reds')
fig.update_layout(
    title={'text': f"Correlation Graph of the Dataset",
           'x': 0.5})
fig.update_layout(coloraxis_showscale=False)
fig.show()

print(df.columns)



print(df.columns)

print(df.columns)

df ['Rating'].value_counts()

"""# **Vectorization by Count Vectorizer, Train-Test Split**"""

# vectorizer = CountVectorizer( stop_words = 'english' )
vectorizer = CountVectorizer()

x = df['Review']
y = df['Rating']


#Test-train split
x_train, x_test, y_train, y_test = train_test_split (x, y, random_state = 42, test_size = 0.20, shuffle = True, stratify = y)

print ("\nTraining Data size: ", x_train.shape)
print ("Testing Data size: ", x_test.shape)

"""# **For 5 labels classification of sentiments**

## **MultinimialNB classifier 5 label: fitting and prediction**
"""

cv_train_transformed = vectorizer.fit_transform(x_train)
cv_test_transformed = vectorizer.transform(x_test)


#MultinomialNB classifier fitting
classifier = MultinomialNB()
classifier.fit(cv_train_transformed, y_train)

y_pred = classifier.predict(cv_test_transformed)


print("True events:", y_test.values[:20])
print("Predicted:  ", y_pred[:20])


#MultinomialNB Prediction
def accuracy(y_pred, y_test):

  score1 = accuracy_score (y_test, y_pred)

  print ("\nMultinomialNB Naive Bayes prediction score for 5labels :", score1)
  print( f'Accuracy of the Model: { round( accuracy_score( y_test, y_pred ) * 100, 2 ) }%' )
  print( "\nclassification report:\n", classification_report( y_test, y_pred ) )

accuracy(y_pred, y_test)

"""### **Confusion Matrix and Heatmap 5 label multinomialNB**"""

# confusion matrix
s = "MultinomialNB (5 labels)"

cm = confusion_matrix ( y_test, y_pred )
print( "\nconfusion matrix:\n", cm )


#Heat-map of confusion matrix
def heatmap (cm, s):
  fig, ax = plot_confusion_matrix( conf_mat = cm, figsize = (10, 10), colorbar = True, show_absolute = True,
                                  show_normed = True, cmap = plt.cm.winter_r )

  plt.xlabel ("True sentiment", fontsize = 14)
  plt.ylabel ("Predicted sentiment", fontsize = 14)
  plt.title ("\nHeat map of confusion matrix Predicted sentiment versus True sentiment \nfor {}".format(s), fontweight = 'bold')

  ax.set_xticklabels((ax.get_xticks() +1).astype(int))
  ax.set_yticklabels((ax.get_yticks() +1).astype(int))

  plt.show ()

heatmap( cm, s )

"""### **Bar-graph 5 label MultinomialNB**"""

# True test values
str = "TRUE"

def Value_counts(str, data):

    values, counts = np.unique(data, return_counts=True)
    print('\n{} sentiments in text sample: {}\n                        Counts: {}\n'.format(str, values, counts))

    return values, counts

true_val = Value_counts(str, y_test)


# Predicted test values
str2 = "PREDICTED"

pred_val = Value_counts(str2, y_pred)



# Plot Bar Graph of true test values vrs predicted values in graph
def plot_bar(values, counts, values2, counts2, s):

    fig, ax = plt.subplots(figsize =(8, 8))

    # set width of bar
    barWidth = 0.35

    # set height of bar
    Values = values
    Counts = counts
    Values2 = values2
    Counts2 = counts2

    # Set position of bar on X axis
    br2 = [ x + barWidth for x in Values2 ]

    plt.bar(Values, Counts, color='brown', edgecolor='black', width=barWidth, label='True')
    plt.bar(br2, Counts2, color='teal', edgecolor='black', width=barWidth, label='Predicted')

    plt.xlabel('Sentiments', fontsize=14)
    plt.ylabel('Frequency of Sentiments', fontsize=14)
    plt.title("Bar graph showing the True values vs Predicted values of the sentiments \nin the test sample for {}".format( s ),
              fontweight ='bold')

    labels = [1, 2, 3, 4, 5]

    plt.xticks(Values, labels)

    tick_spacing = 100
    ax.yaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))

    plt.legend()
    plt.show()


plot_bar(true_val[0], true_val[1], pred_val[0], pred_val[1], s)

"""## **Class Weight Assignment with ComplementNB 5 labels**"""

#Class weight assigning
from sklearn.naive_bayes import ComplementNB
from sklearn.utils import class_weight


weighted_model = ComplementNB()
class_weight = class_weight.compute_class_weight( class_weight = "balanced", classes = np.unique( y_train ), y = y_train )

print(y_train)

print( "\nThe class weights are: ", class_weight )

weighted_model.fit( cv_train_transformed, y_train, sample_weight=[class_weight[i] for i in (y_train - 1)] )

#Prediction after class weight assigning
weighted_pred = weighted_model.predict(cv_test_transformed)


score = accuracy_score (y_test, weighted_pred)

print ("\nComplementNB Naive Bayes prediction score for 5labels :", score)
print( f'Accuracy of the Model: { round( accuracy_score(y_test, weighted_pred) * 100, 2 ) }%' )
print( "\nclassification report:\n", classification_report(y_test, weighted_pred) )

"""### **Confusion matrix & Heat-map 5 label after oversampling**"""

# confusion matrix
s1 = "ComplementNB (5 labels) after Class-weight assignment"

cm = confusion_matrix ( weighted_pred, y_test)
print ('Confusion Matrix: \n', cm)

#plot heat-map for confusion matrix for Naive Bayes
heatmap( cm, s1 )

"""### **Bar-graph 5 label after oversampling**"""

#True Frequency of y_test for 3 labels
str = "Weighted TRUE"

true_val1 = Value_counts(str, y_test)

str2 = "Weighted PREDICTED"

#Predicted frequency of y_test
pred_val1 = Value_counts(str2, weighted_pred)


#Graph
plot_bar(true_val1[0], true_val1[1], pred_val1[0], pred_val1[1], s1)

"""# **For 3 labels classification of sentiments**






"""

label = {
    5: 3,
    4: 3,
    3: 2,
    2: 1,
    1: 1
}

df['sentiment_in_PosNegNeu'] = df['Rating'].apply(lambda x: label[x])
df.head(5)

"""## **MultinimialNB classifier 3 label: fitting and prediction**"""

x_train2, x_test2, y_train2, y_test2 = train_test_split (x, df ['sentiment_in_PosNegNeu'], random_state = 42,
                                                         test_size = 0.20, shuffle=True, stratify = df ['sentiment_in_PosNegNeu'])

cv_train_transformed = vectorizer.fit_transform(x_train2)
cv_test_transformed = vectorizer.transform(x_test2)


#Fitting new y_train with MultinominalNB() classifier
classifier.fit(cv_train_transformed, y_train2)
y_pred2 = classifier.predict(cv_test_transformed)


#new Prediction
accuracy(y_pred2, y_test2)

"""### **Confusion matrix & Heat-map 3 label LR**"""

# confusion matrix
s2 = "MultinomialNB (3 labels)"

cm2 = confusion_matrix ( y_pred2, y_test2 )
print ('Confusion Matrix: \n', cm2)

#plot heat-map for confusion matrix for Naive Bayes
heatmap( cm2, s2 )

"""### **Bar-graph 3 label LR**"""

# True Frequency of y_test for 3 labels
true_val2 = Value_counts(str, y_test2)

# Predicted frequency of y_test
pred_val2 = Value_counts(str2, y_pred2)

# Ensure consistency in unique values
unique_sentiments = np.unique(np.concatenate((true_val2[0], pred_val2[0])))

try:
    plot_bar(unique_sentiments, true_val2[1], unique_sentiments, pred_val2[1], s2)
except ValueError as e:
    pass  # Do nothing and continue execution

"""## **Class-Weight assignment complementNB 3 label**"""

weighted_model = ComplementNB()

# print(y_train2)

# class_weight = class_weight.compute_class_weight( class_weight = "balanced", classes = np.unique( y_train2 ), y = y_train2 )

# weighted_model.fit( cv_train_transformed, y_train2, sample_weight=[class_weight[i] for i in (y_train2 - 1)] )

weighted_model.fit( cv_train_transformed, y_train2 )

#Prediction after class weight assigning
weighted_pred = weighted_model.predict(cv_test_transformed)

score = accuracy_score (y_test2, weighted_pred)

print ("\nComplementNB Naive Bayes prediction score for 5labels :", score)
print( f'Accuracy of the Model: { round( accuracy_score(y_test2, weighted_pred) * 100, 2 ) }%' )
print( "\nclassification report:\n", classification_report(y_test2, weighted_pred) )

"""### **Confusion matrix & Heat-map 3 label after oversampling**"""

# confusion matrix
s2 = "ComplementNB (3 labels) after Class-weight Assignmnet"

cm = confusion_matrix ( weighted_pred, y_test2)
print ('Confusion Matrix: \n', cm)

#plot heat-map for confusion matrix for Naive Bayes
heatmap( cm, s2 )

"""### **Bar-graph 3 label after oversampling**"""

# True Frequency of y_test for 3 labels
true_val3 = Value_counts(str, y_test2)

# Predicted frequency of y_test
pred_val3 = Value_counts(str2, y_pred2)

# Ensure consistency in unique values
unique_sentiments = np.unique(np.concatenate((true_val3[0], pred_val3[0])))

try:
    plot_bar(unique_sentiments, true_val3[1], unique_sentiments, pred_val3[1], s2)
except ValueError as e:
    pass  # Do nothing and continue execution